{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c191c972-2c85-4284-a555-7db618ba5b26",
   "metadata": {},
   "source": [
    "# Laboratorio de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c177c-1848-44b0-8c6b-b35537b5ae2f",
   "metadata": {},
   "source": [
    "|                |   |\n",
    ":----------------|---|\n",
    "| **Nombre**     |  Nicolás Martínez Gutiérrez |\n",
    "| **Fecha**      |  13/10/2025 |\n",
    "| **Expediente** |  751746 | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95344a-b10c-465d-a5f4-bfa65905420c",
   "metadata": {},
   "source": [
    "In machine learning, Support Vector Machines (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It is mostly used in classification problems. In this algorithm, each data item is plotted as a point in p-dimensional space (where p is the number of features), with the value of each feature being the value of a particular coordinate. Then, classification is performed by finding the hyper-plane that best differentiates the two classes (or more if we have a multi class problem):\n",
    "\n",
    "$$ f(x) = w^T \\varphi(x) + b $$\n",
    "\n",
    "where $\\varphi: X \\rightarrow F $ is a function that makes each input point $x$ correspond to a point in F, where F is a Hilbert space.\n",
    "\n",
    "In addition to performing linear classification, SVMs can efficiently perform a non-linear classification, implicitly mapping their inputs into high-dimensional feature spaces (more specifically using the kernel trick, like the RBF funcion). \n",
    "\n",
    "[1]\n",
    "\n",
    "OLS utilizes the squared residuals to fit the parameters. Large residuals caused by outliers may worsen the accuracy significantly.\n",
    "\n",
    "Support Vectors use piecewise linear functions to counter this, in which a hyperparameter  $\\epsilon$ called the margin lets errors that are less or equal to it be 0, and error larger than it be $e - \\epsilon$. \n",
    "\n",
    "The problem to solve is:\n",
    "\n",
    "\\begin{split}\n",
    "        \\min_{w, b, \\xi, \\xi^*} \\mathcal{P}_\\epsilon(w, b, \\xi) &= \\frac{1}{2} w^T w + c \\sum_{k=1}^{N} \\xi_k \\\\\n",
    "        \\text{s.t. } & y_k [w^T \\varphi(x_k) - b] \\geq 1- \\xi_k,\\ \\ k = 1, ..., N \\\\\n",
    "        & \\xi_k \\geq 0,\\ \\ k = 1, ..., N\n",
    "\\end{split}\n",
    "\n",
    "\n",
    "The most important question that arises when using a SVM is how to choose the correct hyperplane. Consider the following scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4c629-c547-433d-90ff-852a1e722a95",
   "metadata": {},
   "source": [
    "### Scenario 1\n",
    "\n",
    "In this scenario there are three hyperplanes called A, B, and C. Now, the problem is to identify the hyperplane which best differentiates the stars and the circles.\n",
    "\n",
    "<center><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/SVM_21-2.png\" alt=\"what image shows\"></center>\n",
    "\n",
    "In this case, hyperplane B separates the stars and the circle betters, hence it is the correct hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d158b5b-99f1-435a-9b91-0d8a17363bce",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "Now take another scenario where all three hyperplanes are segregating classes well. The question that arises is how to choose the best hyperplane in this situation.\n",
    "\n",
    "<center><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/SVM_4-2.png\" alt=\"what image shows\"></center>\n",
    "\n",
    "In such scenarios, we calculate the margin (which is the distance between nearest data point and the hyperplane). The hyperplane with the largest margin will be considered as the correct hyperplane to classify the dataset.\n",
    "\n",
    "Here C has the largest margin. Hence, it is considered as the best hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae4b6d-e3fe-4496-8f7a-242697fb058e",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "Knowing \n",
    "$$ w = \\sum_{k=1}^{N} \\alpha_k y_k \\varphi(x_k) $$\n",
    "\n",
    "And\n",
    "$$ y_{pred} = w^T \\varphi(x) + b $$\n",
    "\n",
    "Then \n",
    "$$ y_{pred} = (\\sum_{k=1}^{N} \\alpha_k y_k \\varphi(x_k))^T \\varphi(x) + b $$\n",
    "\n",
    "Where $\\varphi$ is a function that makes each input in $x$ correspond to a point in $F$ (a Hilbert space). This can be seen as processing and transforming the input featuers to keep the model's convexity. [2]\n",
    "\n",
    "This also allows us to transform the inputs into another space where they might be more easily classified.\n",
    "\n",
    "<center><img src=https://miro.medium.com/max/838/1*gXvhD4IomaC9Jb37tzDUVg.png alt=\"what image shows\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cdd455-cb0d-477c-84a8-326453792426",
   "metadata": {},
   "source": [
    "## ROC and AUC\n",
    "\n",
    "A ROC (Receiver Operating Characteristic) is a graph that shows how the classification model performs at the classification thresholds. \n",
    "\n",
    "ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better. [3]\n",
    "\n",
    "True Positive Rate is a synonym for Recall and defined as:\n",
    "$$ TPR = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "False Positive Rate is a synonym for Specificity and defined as:\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).\n",
    "\n",
    "E.g. If you lower a classification threshold, more items would be classified as positive, increasing False Positives and True Positives.\n",
    "\n",
    "AUC stands for Area under the ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8fe3e-dca0-4f66-94c6-18f3545cb865",
   "metadata": {},
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93403a-dc07-4059-8e97-991e98d8caf4",
   "metadata": {},
   "source": [
    "- Utiliza el dataset `Iris`, modela con SVC y haz Cross-Validation de diferentes kernels ('linear', 'poly', 'rbf', 'sigmoid').\n",
    "- Modela con LogisticRegression.\n",
    "- El método de Cross-Validation es K-Folds con $k=10$.\n",
    "- Utiliza el AUC como métrico de Cross-Validation.\n",
    "- Compara resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedae7bb-ce63-4f38-aeb1-353c7f579909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf935dc-8d6b-4503-8ba3-90d9d55fef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f3396a-82a3-4046-ae16-d88946f8cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "m1, m2, m3 = \"accuracy\", \"f1_macro\", \"roc_auc_ovr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c2f70-a517-46e6-9e0d-7ebf273578fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "svc_poly = SVC(kernel=\"poly\", probability=True, random_state=42)\n",
    "svc_rbf = SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "svc_sigmoid = SVC(kernel=\"sigmoid\", probability=True, random_state=42)\n",
    "logreg = LogisticRegression(max_iter=2000, solver=\"lbfgs\", multi_class=\"ovr\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1081f0e-ad6c-42cb-af2d-efe991232343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "acc_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8cd4b9-cd6d-4b75-859f-de1c4a89eadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "f1_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed93b163-da30-4451-b903-bcec8a6d31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nicolas\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "auc_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31fedc09-9088-475d-9540-b55f32ef16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"Model\": [\"SVC (linear)\", \"SVC (poly)\", \"SVC (rbf)\", \"SVC (sigmoid)\", \"Logistic Regression\"],\n",
    "    \"Accuracy\": [acc_linear, acc_poly, acc_rbf, acc_sigmoid, acc_logreg],\n",
    "    \"F1-macro\": [f1_linear, f1_poly, f1_rbf, f1_sigmoid, f1_logreg],\n",
    "    \"ROC AUC (OvR)\": [auc_linear, auc_poly, auc_rbf, auc_sigmoid, auc_logreg]\n",
    "}).sort_values(\"ROC AUC (OvR)\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24963648-a953-41ce-90bc-6f4ac6ba0487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>ROC AUC (OvR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC (rbf)</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.959731</td>\n",
       "      <td>0.997333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC (linear)</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979377</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC (poly)</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.959175</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.938552</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC (sigmoid)</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  F1-macro  ROC AUC (OvR)\n",
       "0            SVC (rbf)  0.960000  0.959731       0.997333\n",
       "1         SVC (linear)  0.980000  0.979377       0.996000\n",
       "2           SVC (poly)  0.960000  0.959175       0.996000\n",
       "3  Logistic Regression  0.940000  0.938552       0.986667\n",
       "4        SVC (sigmoid)  0.073333  0.047778       0.746667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58736316-bce5-4a87-bc7c-a006a856fbee",
   "metadata": {},
   "source": [
    "Los modelos SVC con kernel lineal y RBF fueron los que obtuvieron el mejor desempeño al clasificar el dataset Iris, con valores de accuracy de 0.98 y 0.96, F1-macro de 0.9794 y 0.9597, y ROC AUC (OvR) de 0.9960 y 0.9973 respectivamente. El SVC con kernel polinomial también tuvo un rendimiento muy alto, con un AUC de 0.9960, mientras que la regresión logística alcanzó resultados ligeramente menores, con un accuracy de 0.94, un F1-macro de 0.9386 y un AUC de 0.9867. En contraste, el SVC con kernel sigmoide tuvo un desempeño deficiente, con un accuracy de apenas 0.0733 y un AUC de 0.7467, mostrando que no logra separar bien las clases. En resumen, los kernels lineal, RBF y polinomial son los más efectivos para este dataset, mientras que el sigmoide no resulta adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e26d1b-1bd2-4839-a743-a63f6b258a42",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "- Repite el ejercicio 1 con el dataset `Default`. Utiliza `default` como target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0bb2af-09ae-4056-aa6f-f15289ae1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Default.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f51dd70-7125-4c3f-a5f3-0f2e56cea8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\AppData\\Local\\Temp\\ipykernel_12184\\3771904528.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = data[\"default\"].replace({\"Yes\": 1, \"No\": 0}).to_numpy()\n"
     ]
    }
   ],
   "source": [
    "y = data[\"default\"].replace({\"Yes\": 1, \"No\": 0}).to_numpy()\n",
    "X_num = (data[[\"balance\", \"income\"]] - data[[\"balance\", \"income\"]].mean()) / data[[\"balance\", \"income\"]].std()\n",
    "X_cat = pd.get_dummies(data[[\"student\"]], drop_first=True)\n",
    "X = pd.concat([X_num, X_cat], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fcc7e9-6329-4c76-baf0-9b45d56f1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "m1, m2, m3 = \"accuracy\", \"f1_macro\", \"roc_auc\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82736db-9841-4045-81dc-1df3f39ead01",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel=\"linear\", probability=True, class_weight=\"balanced\", random_state=42)\n",
    "svc_poly = SVC(kernel=\"poly\", probability=True, class_weight=\"balanced\", random_state=42)\n",
    "svc_rbf = SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42)\n",
    "svc_sigmoid = SVC(kernel=\"sigmoid\", probability=True, class_weight=\"balanced\", random_state=42)\n",
    "logreg = LogisticRegression(max_iter=2000, solver=\"lbfgs\", class_weight=\"balanced\", random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4dcbf2-a2c1-426a-acb4-013aa325f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m1).mean()\n",
    "acc_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23886a0a-9aca-49b8-8184-d4932a010f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m2).mean()\n",
    "f1_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4a68dc-2de2-4a83-ad2c-dc695a86571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_linear = cross_val_score(svc_linear, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_poly = cross_val_score(svc_poly, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_rbf = cross_val_score(svc_rbf, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_sigmoid = cross_val_score(svc_sigmoid, X, y, cv=cv, scoring=m3).mean()\n",
    "auc_logreg = cross_val_score(logreg, X, y, cv=cv, scoring=m3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec6d440f-9770-4f7a-8c31-30f7904fde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"SVC (linear)\", \"SVC (poly)\", \"SVC (rbf)\", \"SVC (sigmoid)\", \"Logistic Regression\"],\n",
    "    \"Accuracy\": [acc_linear, acc_poly, acc_rbf, acc_sigmoid, acc_logreg],\n",
    "    \"F1-macro\": [f1_linear, f1_poly, f1_rbf, f1_sigmoid, f1_logreg],\n",
    "    \"ROC AUC\":  [auc_linear, auc_poly, auc_rbf, auc_sigmoid, auc_logreg]\n",
    "}).sort_values(\"ROC AUC\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78634f04-509a-4c18-a9d7-98abfa0cc06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.8655</td>\n",
       "      <td>0.616271</td>\n",
       "      <td>0.948297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC (linear)</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>0.615288</td>\n",
       "      <td>0.948220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC (poly)</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.629055</td>\n",
       "      <td>0.940819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC (rbf)</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.605436</td>\n",
       "      <td>0.922427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC (sigmoid)</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>0.478599</td>\n",
       "      <td>0.808810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  F1-macro   ROC AUC\n",
       "0  Logistic Regression    0.8655  0.616271  0.948297\n",
       "1         SVC (linear)    0.8646  0.615288  0.948220\n",
       "2           SVC (poly)    0.8822  0.629055  0.940819\n",
       "3            SVC (rbf)    0.8540  0.605436  0.922427\n",
       "4        SVC (sigmoid)    0.7061  0.478599  0.808810"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf7d0f-8381-4a45-8d61-1e63a24b9763",
   "metadata": {},
   "source": [
    "Los resultados don el dataset Default muestran que la Regresión Logística fue el modelo con mejor desempeño general, con un accuracy de 0.8655, F1-macro de 0.6163 y AUC de 0.9483, lo que indica que logra clasificar correctamente la mayoría de los casos y mantiene un buen equilibrio entre precisión y recall. El SVC (lineal) obtuvo resultados muy similares (accuracy 0.8646, AUC 0.9482), lo que lo convierte en una alternativa casi igual de efectiva. El SVC (polinomial) también tuvo un rendimiento competitivo con un AUC de 0.9408, mientras que el SVC (RBF) fue un poco menos preciso (accuracy 0.854, AUC 0.9224). Por otro lado, el SVC (sigmoide) fue claramente el modelo menos eficiente, con un accuracy de 0.706 y un AUC de 0.8088. En resumen, los mejores modelos para este conjunto de datos fueron la Regresión Logística y el SVC lineal, que ofrecen la mejor combinación entre precisión y capacidad para distinguir correctamente los casos de incumplimiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "138473b9-3a81-43cb-83a5-a362e535b736",
   "metadata": {},
   "source": [
    "# Addendum\n",
    "\n",
    "Métricos disponibles para clasificación:\n",
    "- ‘accuracy’\n",
    "- ‘balanced_accuracy’\n",
    "- ‘top_k_accuracy’\n",
    "- ‘average_precision’\n",
    "- ‘neg_brier_score’\n",
    "- ‘f1’\n",
    "- ‘f1_micro’\n",
    "- ‘f1_macro’\n",
    "- ‘f1_weighted’\n",
    "- ‘f1_samples’\n",
    "- ‘neg_log_loss’\n",
    "- ‘precision’ etc.\n",
    "- ‘recall’ etc.\n",
    "- ‘jaccard’ etc.\n",
    "- ‘roc_auc’\n",
    "- ‘roc_auc_ovr’\n",
    "- ‘roc_auc_ovo’\n",
    "- ‘roc_auc_ovr_weighted’\n",
    "- ‘roc_auc_ovo_weighted’\n",
    "- ‘d2_log_loss_score’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a114d-84df-46fc-b1fb-4729410bdb5f",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Shigeo Abe.Support Vector Machines for Pattern Classification,2Ed.Springer-Verlag London,2010. ISBN978-1-84996-097-7. URLhttps://www.springer.com/gp/book/9781849960977.\n",
    "\n",
    "[2] Johan A K Suykens, Tony Van Gestel, Jos De Brabanter, BartDe Moor, and Joos Vandewalle.Least Squares Support VectorMachines. World Scientific,2002. ISBN9789812381514. URLhttps://www.worldscientific.com/worldscibooks/10.1142/5089.\n",
    "\n",
    "[3] Bradley, A. P. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7), 1145-1159. URL https://www.researchgate.net/post/how_can_I_interpret_the_ROC_curve_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
